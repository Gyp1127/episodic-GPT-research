{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1SsKfFCMWEd2JoNUql_sY5dlA-EwxFMSc",
      "authorship_tag": "ABX9TyOvDPv184b9NPFVLVD3JD7V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- get chatgpt to accept prompts\n",
        "- get chatgpt to output prompts"
      ],
      "metadata": {
        "id": "h5xgazSZCxmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXo_aVOWCg3D",
        "outputId": "e8c1c4ba-5d7a-4a63-8924-02eccb12ff9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.34-py3-none-any.whl (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.8.6)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.7.22)\n",
            "Installing collected packages: fastavro, backoff, cohere\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.34 fastavro-1.8.2\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting openai\n",
            "  Downloading openai-1.3.2-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere\n",
        "!pip install tiktoken\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('API_KEY'))\n",
        "MODEL = \"gpt-3.5-turbo-16k\"\n"
      ],
      "metadata": {
        "id": "kVFh97VVNRg_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial call to GPT to understand the trials\n",
        "init_sys_call = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"\"\"Your task is to give judgement about where you were based on the time we give. You will give 4 alternative options. You MUST select one.\n",
        "\n",
        "                  Example:\n",
        "\n",
        "                  Presented:\n",
        "                  1 at 6PM 11/08/2019\n",
        "                  2 at 7PM 12/08/2019\n",
        "                  ...\n",
        "\n",
        "                  Question:\n",
        "                  Where were you at 6PM 11/09/2019:\n",
        "                  A. 2\n",
        "                  B. 3\n",
        "                  C. 1\n",
        "                  D. 30\n",
        "\n",
        "                  Response:\n",
        "                  C\n",
        "\n",
        "                  Where were you at 7PM 12/09/2019:\n",
        "                  A. 2\n",
        "                  B. 3\n",
        "                  C. 9\n",
        "                  D. 30\n",
        "\n",
        "                  Response:\n",
        "                  A\n",
        "                  ...\"\"\"\n",
        "              }]"
      ],
      "metadata": {
        "id": "RanH-xzM2Ag0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events = pd.read_csv(userdata.get('EVENTS_FILE'))\n",
        "experiments = pd.read_csv(userdata.get('EXPERIMENTS_FILE'))"
      ],
      "metadata": {
        "id": "i15Ckz0iTkE7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful constants\n",
        "EVENT_DF = 0\n",
        "EVENT_OFFSET = 1\n",
        "EXPERIMENT_DF = 2\n",
        "EXPERIMENT_OFFSET = 3\n",
        "\n",
        "\n",
        "def create_event_experiment_dfs(uuid):\n",
        "  \"\"\"\n",
        "  create_event_experiment_dfs() creates the appropriate dataframes and index offsets to reference events and experiments.\n",
        "\n",
        "  :param USER_ID: the user's unique identification string\n",
        "  :return: user's events, event offset index, user's experiment, experiment offset index\n",
        "  \"\"\"\n",
        "\n",
        "  events_temp = events[['ID', 'StartDateTime Local', 'GPS Cluster Original']][events['USER ID']==uuid]\n",
        "  experiments_temp = experiments[['ID', 'Target', 'A', 'B', 'C', 'D']][experiments['USER ID']==uuid]\n",
        "\n",
        "  # offsets\n",
        "  events_offset = events_temp.index.min()\n",
        "  experiments_offset = experiments_temp.index.min()\n",
        "  return (events_temp, events_offset, experiments_temp, experiments_offset)\n",
        "\n",
        "temp = create_event_experiment_dfs(\"ap-northeast-1:e7c916fc-736e-47e6-979a-c1c937ebe094\")\n"
      ],
      "metadata": {
        "id": "xOZGHnz8iIU5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let GPT know where it was\n",
        "\n",
        "def gen_location(Event_Exp_df):\n",
        "  \"\"\"\n",
        "  gen_location() turns each entry of event_df as a context for GPT\n",
        "\n",
        "  :param Event_Exp_df: the list of the event_df, event_offset, exp_df, exp_offset\n",
        "  :return: the structured msg containing where a particular participant was located\n",
        "  \"\"\"\n",
        "\n",
        "  events_df = Event_Exp_df[EVENT_DF]\n",
        "  events_offset = Event_Exp_df[EVENT_OFFSET]\n",
        "  location_data = []\n",
        "\n",
        "  # Each entry is a context for GPT\n",
        "  for i in range(len(events_df)):\n",
        "    datetime = events_df.loc[i+events_offset, \"StartDateTime Local\"]\n",
        "    date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})T\"\n",
        "    time_pattern = r\"\\d{4}-\\d{2}-\\d{2}T(\\d{2}:\\d{2}:\\d{2})Z\"\n",
        "    date = re.search(date_pattern, datetime).group(1)\n",
        "    time = re.search(time_pattern, datetime).group(1)\n",
        "\n",
        "    gps_cluster = events_df.loc[i+events_offset, \"GPS Cluster Original\"]\n",
        "    location_data.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"{gps_cluster} at {time} {date}\"\n",
        "    })\n",
        "\n",
        "  return location_data\n",
        "\n",
        "\n",
        "  # 2019-08-11T06:00:00Z\n",
        "  #  print(events_1.loc[i, \"ID\"], events_1.loc[i, \"GPS Cluster Original\"])\n",
        "temp_loc = gen_location(temp)"
      ],
      "metadata": {
        "id": "cPb-Ypmr65vt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Form Questions for GPT\n",
        "\n",
        "def gen_questions(Event_Exp_df):\n",
        "  \"\"\"\n",
        "  gen_questions() generates the questions that will be fed to GPT by using the\n",
        "  experiment_df\n",
        "\n",
        "  :param Event_Exp_df: the list of the event_df, event_offset, exp_df, exp_offset\n",
        "  :return: the structured msg containing questions from the experiment for a\n",
        "           given participant\n",
        "  \"\"\"\n",
        "\n",
        "  events_df = Event_Exp_df[EVENT_DF]\n",
        "  experiments_df = Event_Exp_df[EXPERIMENT_DF]\n",
        "  experiments_offset = Event_Exp_df[EXPERIMENT_OFFSET]\n",
        "  question_data = []\n",
        "\n",
        "  # Turn each entry into an input for GPT\n",
        "  for i in range(len(experiments_df)):\n",
        "    question = \"\"\n",
        "    response_id = experiments_df.loc[i+experiments_offset, \"Target\"]\n",
        "    correct_event_id = experiments_df.loc[i+experiments_offset, response_id]\n",
        "\n",
        "    # Perform lookup for experiments_df from events_df\n",
        "    cluster_question = []\n",
        "    for alpha in ['A', 'B', 'C', 'D']:\n",
        "      alpha_response_id = experiments_df.loc[i+experiments_offset, alpha]\n",
        "      cluster_question.append(events_df.loc[alpha_response_id, \"GPS Cluster Original\"])\n",
        "\n",
        "    # Datetime processing\n",
        "    datetime = events_df.loc[correct_event_id, \"StartDateTime Local\"]\n",
        "    date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})T\"\n",
        "    time_pattern = r\"\\d{4}-\\d{2}-\\d{2}T(\\d{2}:\\d{2}:\\d{2})Z\"\n",
        "    date = re.search(date_pattern, datetime).group(1)\n",
        "    time = re.search(time_pattern, datetime).group(1)\n",
        "\n",
        "    # Question forming\n",
        "    question += f\"Where were you at {time} {date}:\\n\"\n",
        "    question += f\"A. {cluster_question[0]}\\n\"\n",
        "    question += f\"B. {cluster_question[1]}\\n\"\n",
        "    question += f\"C. {cluster_question[2]}\\n\"\n",
        "    question += f\"D. {cluster_question[3]}\"\n",
        "\n",
        "    question_data.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": question\n",
        "    })\n",
        "\n",
        "  return question_data\n",
        "\n",
        "temp_ques = gen_questions(temp)"
      ],
      "metadata": {
        "id": "1uVAMmwDlvQx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing (per 1k token) is specific for this model: GPT 3.5 Turbo (16k)\n",
        "gpt_pricing = {\"input\": 0.001, \"output\": 0.002}\n",
        "\n",
        "def run_gpt(Event_Exp_df, location_data, question_data):\n",
        "  \"\"\"\n",
        "  run_gpt() is the main code for interacting with the OpenAI API to obtain expected output\n",
        "\n",
        "  :param Event_Exp_df: the list of the event_df, event_offset, exp_df, exp_offset\n",
        "  :param location_data: the structured location information for GPT\n",
        "  :param question_data: the structured questions for GPT\n",
        "  :return: an array of output as specified by the experiment\n",
        "  \"\"\"\n",
        "\n",
        "  experiments_df = Event_Exp_df[EXPERIMENT_DF]\n",
        "  output_msg = []\n",
        "  message = {}\n",
        "  total_cost = 0\n",
        "\n",
        "  for i in range(len(experiments_df)):\n",
        "    message = init_sys_call #+ location_data\n",
        "\n",
        "    # Append previous questions as context\n",
        "    if (i==0):\n",
        "      message.append(question_data[0])\n",
        "    else:\n",
        "      for j in range(i+1):\n",
        "        message.append(question_data[j])\n",
        "        if (j<i):\n",
        "          message.append({'role': 'assistant', 'content': output_msg[j]})\n",
        "\n",
        "    # Generate response body for API\n",
        "    response = client.chat.completions.create(\n",
        "      model=MODEL,\n",
        "      messages=message\n",
        "    )\n",
        "\n",
        "    # Notify if it's too large for current model\n",
        "    if (response.choices[0].finish_reason == \"length\"):\n",
        "      print(\"ERROR: Too Long\")\n",
        "      return output_msg\n",
        "\n",
        "    # Obtain results and save it as context for future questions\n",
        "    output_msg.append(response.choices[0].message.content)\n",
        "\n",
        "    # Cost Calculations\n",
        "    input_cost = gpt_pricing[\"input\"] * response.usage.prompt_tokens\n",
        "    output_cost = gpt_pricing[\"output\"] * response.usage.completion_tokens\n",
        "    total_cost = input_cost + output_cost\n",
        "\n",
        "    break\n",
        "\n",
        "  print(total_cost)\n",
        "\n",
        "  return output_msg\n",
        "\n"
      ],
      "metadata": {
        "id": "PsXBCc4cfftz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = run_gpt(temp, temp_loc, temp_ques)\n",
        "output\n",
        "\n",
        "# ChatCompletion(id='chatcmpl-8LiuiS7iuwFLoZaG86qaBk54NHPDT',\n",
        "#                choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='B. 14', role='assistant', function_call=None, tool_calls=None))],\n",
        "#                created=1700188228, model='gpt-3.5-turbo-16k-0613',\n",
        "#                object='chat.completion',\n",
        "#                system_fingerprint=None,\n",
        "#                usage=CompletionUsage(completion_tokens=4, prompt_tokens=7661, total_tokens=7665))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8DiCG6orcBF",
        "outputId": "0cd0267e-d825-4daa-96ca-eee458924ad9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.262\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A. 27']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(output_msg, columns=[\"Response\"])\n",
        "df.to_csv('/content/data/responseFULLparticipant7.csv', index=True)"
      ],
      "metadata": {
        "id": "VIaitvD5tPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1FEzh5aNlqC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOs\n",
        "- [x] Change all the code into functions\n",
        "- [x] Migrate code for new version of GPT API\n",
        "- [ ] Amend all temporal information to be more human friendly\n",
        "\n",
        "# Prev Minutes\n",
        "- compare human against gpt\n",
        "- gpt not nearly as accurate as human\n",
        "\n",
        "# Model tuning\n",
        "- Question & content:\n",
        "  - convert dates to day (frame questions in terms on weeks)\n",
        "  - x weeks ago, on monday at this time\n",
        "  - change time to e.g., 6am\n",
        "  - change cluster to location 0 (and include the word in questions) (sidebar - diff between place & location)\n",
        "\n",
        "- sequential dependencies: what happens if you dont give it the prev questions\n",
        "  - llms have tendency to repeat itself\n",
        "\n",
        "- Assume midnight observations are at home\n",
        "\n",
        "Then\n",
        "- clustering:\n",
        "  - clusters don't seem like locations\n",
        "  - can we get gps to decode?\n",
        "    - pro: semantically meaningful name insteaad of a cluster num\n",
        "    - cons: error in conversion since it's not accurate\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GIbEz4UHv8oj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HyMf79arwXv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}