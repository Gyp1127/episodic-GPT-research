{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJkk1tiPR4G3a50Rreydn9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- get chatgpt to accept prompts\n",
        "- get chatgpt to output prompts"
      ],
      "metadata": {
        "id": "h5xgazSZCxmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXo_aVOWCg3D",
        "outputId": "f81c4916-b6d5-4aff-be36-8694ac9fc138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.0-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "openai.api_key = userdata.get('API_KEY')\n",
        "MODEL = \"gpt-3.5-turbo-16k\""
      ],
      "metadata": {
        "id": "kVFh97VVNRg_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial call to GPT to understand the trials\n",
        "init_sys_call = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"\"\"Your task is to give judgement about where you were based on the time we give. You will give 4 alternative options. You MUST select one.\n",
        "\n",
        "                  Example:\n",
        "\n",
        "                  Presented:\n",
        "                  1 at 6PM 11/08/2019\n",
        "                  2 at 7PM 12/08/2019\n",
        "                  ...\n",
        "\n",
        "                  Question:\n",
        "                  Where were you at 6PM 11/09/2019:\n",
        "                  A. 2\n",
        "                  B. 3\n",
        "                  C. 1\n",
        "                  D. 30\n",
        "\n",
        "                  Response:\n",
        "                  C\n",
        "\n",
        "                  Where were you at 7PM 12/09/2019:\n",
        "                  A. 2\n",
        "                  B. 3\n",
        "                  C. 9\n",
        "                  D. 30\n",
        "\n",
        "                  Response:\n",
        "                  A\n",
        "                  ...\"\"\"\n",
        "              }]"
      ],
      "metadata": {
        "id": "RanH-xzM2Ag0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events = pd.read_csv(userdata.get('EVENTS_FILE'))\n",
        "experiments = pd.read_csv(userdata.get('EXPERIMENTS_FILE'))\n",
        "\n",
        "# # Base Case Events - 1st User's ID, StartDateTime UTC, GPS Cluster Original\n",
        "# USER_ID = \"ap-northeast-1:2e3dfd5b-19ff-4b99-abe5-bbee4d46749f\"\n",
        "# events_1 = events[['ID', 'StartDateTime Local', 'GPS Cluster Original']][events['USER ID']==USER_ID]\n",
        "\n",
        "\n",
        "# # Base Case Experiment - 1st ID, Target, A, B, C, D\n",
        "# experiments_1 = experiments[['ID', 'Target', 'A', 'B', 'C', 'D']][experiments['USER ID']==USER_ID]\n",
        "\n",
        "# # offsets\n",
        "# events_1_offset = events_1.index.min()\n",
        "# experiments_1_offset = experiments_1.index.min()\n",
        "# experiments_1\n"
      ],
      "metadata": {
        "id": "i15Ckz0iTkE7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful constants\n",
        "EVENT_DF = 0\n",
        "EVENT_OFFSET = 1\n",
        "EXPERIMENT_DF = 2\n",
        "EXPERIMENT_OFFSET = 3\n",
        "\n",
        "\n",
        "def create_event_experiment_dfs(uuid):\n",
        "  \"\"\"\n",
        "  create_event_experiment_dfs() creates the appropriate dataframes and index offsets to reference events and experiments.\n",
        "\n",
        "  :param USER_ID: the user's unique identification string\n",
        "  :return: user's events, event offset index, user's experiment, experiment offset index\n",
        "  \"\"\"\n",
        "\n",
        "  events_temp = events[['ID', 'StartDateTime Local', 'GPS Cluster Original']][events['USER ID']==uuid]\n",
        "  experiments_temp = experiments[['ID', 'Target', 'A', 'B', 'C', 'D']][experiments['USER ID']==uuid]\n",
        "\n",
        "  # offsets\n",
        "  events_offset = events_temp.index.min()\n",
        "  experiments_offset = experiments_temp.index.min()\n",
        "  return (events_temp, events_offset, experiments_temp, experiments_offset)\n",
        "\n",
        "temp = create_event_experiment_dfs(\"ap-northeast-1:e7c916fc-736e-47e6-979a-c1c937ebe094\")\n"
      ],
      "metadata": {
        "id": "xOZGHnz8iIU5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let GPT know where it was\n",
        "\n",
        "def gen_location(Event_Exp_df):\n",
        "  \"\"\"\n",
        "  gen_location() generates the location information of the given data\n",
        "  \"\"\"\n",
        "\n",
        "  events_df = Event_Exp_df[EVENT_DF]\n",
        "  events_offset = Event_Exp_df[EVENT_OFFSET]\n",
        "\n",
        "  location_data = []\n",
        "  for i in range(len(events_df)):\n",
        "    datetime = events_df.loc[i+events_offset, \"StartDateTime Local\"]\n",
        "    date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})T\"\n",
        "    time_pattern = r\"\\d{4}-\\d{2}-\\d{2}T(\\d{2}:\\d{2}:\\d{2})Z\"\n",
        "    date = re.search(date_pattern, datetime).group(1)\n",
        "    time = re.search(time_pattern, datetime).group(1)\n",
        "\n",
        "    gps_cluster = events_df.loc[i+events_offset, \"GPS Cluster Original\"]\n",
        "    location_data.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"{gps_cluster} at {time} {date}\"\n",
        "    })\n",
        "\n",
        "  return location_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # 2019-08-11T06:00:00Z\n",
        "  #  print(events_1.loc[i, \"ID\"], events_1.loc[i, \"GPS Cluster Original\"])\n",
        "temp_loc = gen_location(temp)"
      ],
      "metadata": {
        "id": "cPb-Ypmr65vt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Form Questions for GPT\n",
        "\n",
        "def gen_questions(Event_Exp_df):\n",
        "\n",
        "  events_df = Event_Exp_df[EVENT_DF]\n",
        "  experiments_df = Event_Exp_df[EXPERIMENT_DF]\n",
        "  experiments_offset = Event_Exp_df[EXPERIMENT_OFFSET]\n",
        "\n",
        "  question_data = []\n",
        "  for i in range(len(experiments_df)):\n",
        "    question = \"\"\n",
        "    response_id = experiments_df.loc[i+experiments_offset, \"Target\"]\n",
        "    correct_event_id = experiments_df.loc[i+experiments_offset, response_id]\n",
        "\n",
        "    # Cluster for A, B, C, D respectively\n",
        "    cluster_question = []\n",
        "    for alpha in ['A', 'B', 'C', 'D']:\n",
        "      alpha_response_id = experiments_df.loc[i+experiments_offset, alpha]\n",
        "      cluster_question.append(events_df.loc[alpha_response_id, \"GPS Cluster Original\"])\n",
        "\n",
        "    datetime = events_df.loc[correct_event_id, \"StartDateTime Local\"]\n",
        "    date_pattern = r\"(\\d{4}-\\d{2}-\\d{2})T\"\n",
        "    time_pattern = r\"\\d{4}-\\d{2}-\\d{2}T(\\d{2}:\\d{2}:\\d{2})Z\"\n",
        "    date = re.search(date_pattern, datetime).group(1)\n",
        "    time = re.search(time_pattern, datetime).group(1)\n",
        "\n",
        "    question += f\"Where were you at {time} {date}:\\n\"\n",
        "    question += f\"A. {cluster_question[0]}\\n\"\n",
        "    question += f\"B. {cluster_question[1]}\\n\"\n",
        "    question += f\"C. {cluster_question[2]}\\n\"\n",
        "    question += f\"D. {cluster_question[3]}\"\n",
        "\n",
        "    question_data.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": question\n",
        "    })\n",
        "\n",
        "  return question_data\n",
        "\n",
        "temp_ques = gen_questions(temp)"
      ],
      "metadata": {
        "id": "1uVAMmwDlvQx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_gpt(Event_Exp_df, location_data, question_data):\n",
        "\n",
        "  experiments_df = Event_Exp_df[EXPERIMENT_DF]\n",
        "  output_msg = []\n",
        "  message = {}\n",
        "\n",
        "  for i in range(len(experiments_df)):\n",
        "    message = init_sys_call + location_data\n",
        "\n",
        "    if (i==0):\n",
        "      message.append(question_data[0])\n",
        "    else:\n",
        "      for j in range(i+1):\n",
        "        message.append(question_data[j])\n",
        "        if (j<i):\n",
        "          message.append({'role': 'assistant', 'content': output_msg[j]})\n",
        "\n",
        "    # output_msg.append('A')\n",
        "    # print(message)\n",
        "    response = openai.chat.completions.create(\n",
        "      model=MODEL,\n",
        "      messages=message\n",
        "    )\n",
        "\n",
        "    # print(response)\n",
        "\n",
        "    if (response['choices'][0]['finish_reason'] == \"length\"):\n",
        "      print(\"ERROR: Too Long\")\n",
        "      break\n",
        "\n",
        "    output_msg.append(response['choices'][0]['message']['content'])\n",
        "\n",
        "  return output_msg\n",
        "\n"
      ],
      "metadata": {
        "id": "PsXBCc4cfftz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(output_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjPrhttfqapo",
        "outputId": "23d2175c-231f-41a9-e137-772dbd962d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XcVwaVkmt9b",
        "outputId": "6e31c894-b8f6-46ab-cdf9-0ae28a06e8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-8ABJAQupInduw6mDMTI5bUmAkALhZ at 0x7bcc5275d8a0> JSON: {\n",
              "  \"id\": \"chatcmpl-8ABJAQupInduw6mDMTI5bUmAkALhZ\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1697437440,\n",
              "  \"model\": \"gpt-3.5-turbo-16k-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"B\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 10738,\n",
              "    \"completion_tokens\": 1,\n",
              "    \"total_tokens\": 10739\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "uwAjCQAktBeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(output_msg, columns=[\"Response\"])\n",
        "df.to_csv('/content/data/responseFULLparticipant7.csv', index=True)"
      ],
      "metadata": {
        "id": "VIaitvD5tPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = run_gpt(temp, temp_loc, temp_ques)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "u8DiCG6orcBF",
        "outputId": "ee72e45f-43a1-434c-deb7-c07118dc1514"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-6c93ea2c3832>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_ques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-bc6f1375cd96>\u001b[0m in \u001b[0;36mrun_gpt\u001b[0;34m(Event_Exp_df, location_data, question_data)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# print(response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'finish_reason'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR: Too Long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOs\n",
        "- [x] Change all the code into functions\n",
        "- [ ] Migrate code for new version of GPT API\n",
        "- [ ] Amend all temporal information to be more human friendly\n",
        "\n",
        "# Prev Minutes\n",
        "- compare human against gpt\n",
        "- gpt not nearly as accurate as human\n",
        "\n",
        "# Model tuning\n",
        "- Question & content:\n",
        "  - convert dates to day (frame questions in terms on weeks)\n",
        "  - x weeks ago, on monday at this time\n",
        "  - change time to e.g., 6am\n",
        "  - change cluster to location 0 (and include the word in questions) (sidebar - diff between place & location)\n",
        "\n",
        "- sequential dependencies: what happens if you dont give it the prev questions\n",
        "  - llms have tendency to repeat itself\n",
        "\n",
        "- Assume midnight observations are at home\n",
        "\n",
        "Then\n",
        "- clustering:\n",
        "  - clusters don't seem like locations\n",
        "  - can we get gps to decode?\n",
        "    - pro: semantically meaningful name insteaad of a cluster num\n",
        "    - cons: error in conversion since it's not accurate\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GIbEz4UHv8oj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HyMf79arwXv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}